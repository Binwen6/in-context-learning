@misc{bhattamishra2023understandingincontextlearningtransformers,
      title={Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions}, 
      author={Satwik Bhattamishra and Arkil Patel and Phil Blunsom and Varun Kanade},
      year={2023},
      eprint={2310.03016},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03016}, 
}
@misc{sun2025incontextlearningpolynomialkernel,
      title={In-Context Learning of Polynomial Kernel Regression in Transformers with GLU Layers}, 
      author={Haoyuan Sun and Ali Jadbabaie and Navid Azizan},
      year={2025},
      eprint={2501.18187},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.18187}, 
}
@misc{cole2025incontextlearninglineardynamical,
      title={In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation}, 
      author={Frank Cole and Yulong Lu and Tianhao Zhang and Yuxuan Zhao},
      year={2025},
      eprint={2502.08136},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.08136}, 
}
@misc{garg2023transformerslearnincontextcase,
      title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes}, 
      author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
      year={2023},
      eprint={2208.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.01066}, 
}
@misc{oko2024pretrainedtransformerefficientlylearns,
      title={Pretrained transformer efficiently learns low-dimensional target functions in-context}, 
      author={Kazusato Oko and Yujin Song and Taiji Suzuki and Denny Wu},
      year={2024},
      eprint={2411.02544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.02544}, 
}
@misc{guo2023transformerslearnincontextsimple,
      title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations}, 
      author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
      year={2023},
      eprint={2310.10616},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.10616}, 
}
@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}
@misc{cole2024provable,
    title={Provable In-Context Learning of Linear Systems and Linear Elliptic PDEs with Transformers},
    author={Frank Cole and Yulong Lu and Riley O'Neill and Tianhao Zhang},
    year={2024},
    eprint={2409.12293},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}




