### Mamba
##### 通过试探性实验，即使用含有三种函数类型测试点的混合数据集来训练同一个模型，我们发现：
*模型可能隐含具有对非线性结构更友好的 inductive bias，这或许导致其在处理 Gaussian 和 Dynamical 类函数时的 ICL 表现优于 Linear 函数。*

##### 以核回归（kernel regression）为例

当函数属于某个核（尤其是高斯核）再生核希尔伯特空间（RKHS），最优的点估计可写作：  
这正是 Nadaraya–Watson 核回归（或说高斯过程的后验均值解） ﹣Han et al.（2023）也从贝叶斯视角证明，在示例数足够大时，ICL 可渐近等价于核回归 

为什么Mamba 评估曲线几乎水平且误差极低？  
“一次性”闭式解：Mamba 在预训练里已经把上述核回归的计算路径“写进”了注意力权重（包括查询、键、值投影矩阵）。因此，当你添加更多 in‑context 样本时，并不会像迭代优化那样持续改进解，而是每次都“直接”给出同一个闭式解——误差自然保持平稳。  
数据无噪声、函数类匹配：如果你的测试函数恰好落在 Mamba 训练时使用的函数分布（如高斯核回归）里，在无噪声条件下，理想的核回归插值能几乎零误差地重构输出。


在你切换到 **nonlinear dynamics** 任务时，Mamba 的 in‑context 学习就不再是“直接拿来就用”的闭式解了，而是演化成了一种**渐进式拟合**（渐进式算法推理）过程，因而误差会随着上下文示例数增加而***持续下降***。具体来看：

1. **函数超出了线性／高斯核再生希尔伯特空间**  
   你原来的任务（线性回归 或 高斯核回归）恰好落在了 Mamba 已经“内置”注意力闭式解的范畴，于是它能一口气给出近乎完美的插值解；  
   而 nonlinear dynamics 任务本质更复杂，单靠一个固定的核或最小二乘公式无法一次到位。此时，Transformer 会把注意力＋FFN 结构“编译”成多步梯度／核回归算法，在上下文中逐步迭代逼近最优解 citeturn0search4turn0academia12。

2. **相当于在做多步梯度下降或多项式核回归**  
   - Von Oswald et al. (2023) 证明，Transformer 内部可以实现对深度表示上的线性模型执行**梯度下降**，从而完成复杂任务的拟合 citeturn0search4；  
   - 最新工作也表明，加入 GLU/FNN 层后，Transformer 能执行**多项式核回归**（polynomial kernel regression），进一步扩展了它对非线性函数族的 in‑context 学习能力 citeturn0academia12。

3. **示例越多，逼近越精确，所以误差稳步下降**  
   因为模型相当于在做一个「看多少数据就往最优解走多远」的过程，所以随着示例数量从 10、20、…到 100，Mamba 都在不断优化预测，误差才会从几百一路跌到十几、几甚至更低，这也对应了第二张图里 Mamba 曲线逐渐靠近 Least Squares、3‑NN、XGBoost、2‑layer NN 等基线的现象。

---

**小结**：  
- 对于**simple kernel／线性**任务，Mamba 拥有一次性闭式解，误差平坦；  
- 对于**nonlinear dynamics**，它必须通过“多步”算法推理（本质上是梯度下降或高阶核回归），示例越多就越能拟合、误差越低。  
因此，你看到的误差递减曲线正是 Mamba 在 in‑context 中逐步执行“拟合算法”的天然表现，不是实验出错，而是模型算法能力的另一种「指纹」。
